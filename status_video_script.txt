In the modern world, people use autonomous robots to do some work. Autonomous vacuum robots today commonly exist in many homes, it uses sensors for obstacle detection. A new feature of cliff detection has been used as an additional sensing technology for safety. Under the development of technology, we believe that auto vacuum robots should have the ability to detect cliff and down the stairs safely. Now I'm showing you the map and gameplay of our project. It is a downhill survival game in Minecraft, which asks the agent to go down the map as many levels as possible within limited steps. The agent is able to look down five floors in order to determine the next step. Once the player reaches the bottom of the map, the map will be regenerated. Rewards are given for each level the player goes down, and penalties are applied for touching the map boundary and dropping more than five levels at a time. The agent is expected to have higher rewards for optimization, which is considered to be a combinated evaluation of less falling damages and fewer steps.

Different from path searching game and item collecting game, we are trying to build up a universal strategy that can deal with a randomized map. Currently, we are implementing the game based on Q-network with PyTorch, and the map is build based on the most basic needs. Where we need a map that is easy to observe and achievable for AI training. Therefore, instead of making a 3D map, our initial map is a 2D version while the player can either go left or right. Rewards and Penalties are also applied through XML and loss functions. Where the rewards are the reward of doing downhill effectively and the reward of achieving goals, the penalties are the penalty of moving toward the boundary and of falling over five floors. In the specific, we found that the penalty setting of moving toward boundaries affected our performance a lot. Before applying such a penalty, we receive a result like this. It shows we got all negative rewards under current conditions, where the negative results are from penalties of unexpected falling and an unreachable goal point due to wasting too many steps on hitting the wall. By applying the penalty of moving toward the wall, we finally get the result of the first "successful" try. As you can see, this time we get all the positive results for training with ten thousand times. However, the graph of the training result still has some problems. First, it doesn't have a continuously ascending trend, which could be resulted in the setting of difficulties in our game. We may need to create more complicated challenges for our AI agent. Second, it doesn't have a stable reward value range, everything looks a little bit random, we think the problem could be the setting value of rewards and penalties. We gonna try more possible value in the next stage. Third, it doesn't keep a robust status of doing powerful calculations in the last third part. We believe that there must be a great space for optimization and improvement. We are in the progress of upgrading the current one, hope we can bring a powerful robot in the next report. Thank you for watching.


 